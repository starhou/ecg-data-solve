{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mynetv2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starhou/images-inpainting-with-gan/blob/master/context_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn6DCy__dC6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/Colab Notebooks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsq1uB-8dHj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P7whJQRVe_1",
        "colab_type": "text"
      },
      "source": [
        "###导入相关包"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO0UUxJrdXew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzxcjFKe0FIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "from IPython import display\n",
        "import pylab as pl\n",
        "\n",
        "import pickle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGFvahzahM8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.VERSION)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C_wTmiIWANy",
        "colab_type": "text"
      },
      "source": [
        "###从文件夹下读取数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjsTWF_jjgpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset_path = 'data/lsun_trainset.pickle'\n",
        "testset_path  = 'data/lsun_testset.pickle'\n",
        "dataset_path = 'data/data1/'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jsq24jRt9tn",
        "colab_type": "text"
      },
      "source": [
        "读取训练集和测试集的路径"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7qhzsGdoNd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lsun_images = []\n",
        "for _, _, dir, in os.walk(dataset_path):\n",
        "  lsun_images.extend(dir)\n",
        "\n",
        "lsun_images = list(map(lambda x: dataset_path+x, lsun_images))  \n",
        "lsun_images = np.array(lsun_images)\n",
        "\n",
        "trainset = lsun_images[:int(len(lsun_images)*0.9)]\n",
        "testset = lsun_images[int(len(lsun_images)*0.9):]\n",
        "trainset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUG0r-ykzipq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  img = tf.image.resize(img, new_shape)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM13RM1CbmnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-BIBAd4bnsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num = 0\n",
        "for imgpath in trainset[:2000]:\n",
        "  img = load_img(imgpath)\n",
        "  if num==0:\n",
        "    train_data = tf.stack(img, axis=0)\n",
        "  else:\n",
        "    train_data = tf.concat([train_data,img], axis=0)\n",
        "  num+=1;\n",
        "\n",
        "  \n",
        "num=0\n",
        "for imgpath in testset[:100]:\n",
        "  img = load_img(imgpath)\n",
        "  if num==0:\n",
        "    test_data = tf.stack(img, axis=0)\n",
        "  else:\n",
        "    test_data = tf.concat([test_data,img], axis=0)\n",
        "  num+=1;\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hBKjTwFpLqh",
        "colab_type": "text"
      },
      "source": [
        "读取数据并序列化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSGDncprqk0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = open('train.pkl', 'wb')\n",
        "pickle.dump(train_data, output)\n",
        "output.close()\n",
        "\n",
        "output = open('test.pkl', 'wb')\n",
        "pickle.dump(test_data, output)\n",
        "output.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ554XPfWKTV",
        "colab_type": "text"
      },
      "source": [
        "###定义输入管道"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOVQEAODg-Pu",
        "colab_type": "text"
      },
      "source": [
        "加载数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKz_iC1Yrq-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pkl_file = open('train.pkl', 'rb')\n",
        "train_data = pickle.load(pkl_file)\n",
        "pkl_file.close()\n",
        "print(np.shape(train_data))\n",
        "\n",
        "pkl_file = open('test.pkl', 'rb')\n",
        "test_data = pickle.load(pkl_file)\n",
        "pkl_file.close()\n",
        "print(np.shape(test_data))\n",
        "type(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTGQ-EZXsbJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = tf.image.resize(train_data, (200, 200))\n",
        "test_data = tf.image.resize(test_data, (200, 200))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFaeN8a6hLTd",
        "colab_type": "text"
      },
      "source": [
        "随机生成破损图像"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkwRzKRB0Oen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_randomly(imgs):\n",
        "  \n",
        "  mask_height = 65\n",
        "  mask_width = 65\n",
        "  channels = 3\n",
        "  img_rows = imgs[0].shape[1]\n",
        "\n",
        "  y1 = np.random.randint(0, img_rows - mask_height, len(imgs))\n",
        "  y2 = y1 + mask_height\n",
        "  x1 = np.random.randint(0, img_rows -mask_width, len(imgs))\n",
        "  x2 = x1 + mask_width\n",
        "\n",
        "  masked_imgs = np.empty_like(imgs)\n",
        "  missing_parts = np.empty((len(imgs), mask_height, mask_width, channels))\n",
        "  real_parts = np.empty((len(imgs), mask_height, mask_width, channels))\n",
        "  for i, img in enumerate(imgs):\n",
        "    masked_img = img.copy()\n",
        "    _y1, _y2, _x1, _x2 = y1[i], y2[i], x1[i], x2[i]\n",
        "    missing_parts[i] = masked_img[_y1:_y2, _x1:_x2, :].copy()\n",
        "    real_parts[i] = masked_img[_y1:_y2, _x1:_x2, :].copy()\n",
        "    masked_img[_y1:_y2, _x1:_x2, :] = 0\n",
        "    masked_imgs[i] = masked_img\n",
        "\n",
        "  return masked_imgs,missing_parts,real_parts,(y1, y2, x1, x2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENO23U6S9SEW",
        "colab_type": "text"
      },
      "source": [
        "准备好训练集和测试集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ziq6epzWIvc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BUF = 10\n",
        "BATCH_SIZE = 1\n",
        "mytrain_data = tf.data.Dataset.from_tensor_slices(train_data).shuffle(TRAIN_BUF).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKNwruUi1saO",
        "colab_type": "text"
      },
      "source": [
        "###创建网络\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O6l-KtRhUtq",
        "colab_type": "text"
      },
      "source": [
        "构建网络  生成器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fAg5d2Yr5qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator():\n",
        "  generate = tf.keras.Sequential(\n",
        "    [   #encoder\n",
        "        tf.keras.layers.InputLayer(input_shape=(200, 200, 3)),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=64, kernel_size=3, strides=(2, 2), activation='relu',padding='same'),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=16, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=8, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=4, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "\n",
        "        # mid contact layer\n",
        "        tf.keras.layers.Dense(4),\n",
        "\n",
        "        #decoder\n",
        "        tf.keras.layers.UpSampling2D(),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=4, kernel_size=3,  activation='relu',padding='same'),\n",
        "        tf.keras.layers.UpSampling2D(),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=8, kernel_size=3,  activation='relu'),\n",
        "        tf.keras.layers.UpSampling2D(),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=16, kernel_size=3, activation='relu'),\n",
        "        tf.keras.layers.UpSampling2D(),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32, kernel_size=3,  activation='relu'),\n",
        "        tf.keras.layers.UpSampling2D(),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=3, kernel_size=3, strides=(2, 2),activation='relu'),    \n",
        "\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return generate\n",
        "    \n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXKFAJRKAAff",
        "colab_type": "text"
      },
      "source": [
        "判别器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoftD0Njur7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator():\n",
        "  discriminate = tf.keras.Sequential(\n",
        "      [\n",
        "        tf.keras.layers.InputLayer(input_shape=(65,65,3)),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32, kernel_size=3, activation='relu',padding='same'),\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=16, kernel_size=3, activation='relu',padding='same'),\n",
        "        # No activation\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=8, kernel_size=3,  activation='relu',padding='same'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "      ]\n",
        "  )\n",
        "  return discriminate\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Ysp2iShhAK",
        "colab_type": "text"
      },
      "source": [
        "实例化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ei0Ume2_QT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = generator()\n",
        "discriminator = discriminator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e66VXMdvhifi",
        "colab_type": "text"
      },
      "source": [
        "定义损失函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sya7uZR42_t1",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, generated_output):\n",
        "    \"\"\"The disciminator loss function.\"\"\"\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    return bce(tf.ones_like(real_output), real_output) + bce(tf.zeros_like(generated_output), generated_output)\n",
        "\n",
        "\n",
        "def generator_loss(real_parts, generated_images,generated_output):\n",
        "    \"\"\"The Generator loss function.\"\"\"\n",
        "    bce = tf.keras.losses.MeanSquaredError()\n",
        "    cce = tf.keras.losses.BinaryCrossentropy()\n",
        "    return bce(real_parts, generated_images)+ cce(tf.ones_like(generated_output), generated_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0emXQyqYhlga",
        "colab_type": "text"
      },
      "source": [
        "定义优化器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt-rFItABSLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.train.AdamOptimizer(1e-4)\n",
        "discriminator_optimizer = tf.train.AdamOptimizer(1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po8CmYEJhlAq",
        "colab_type": "text"
      },
      "source": [
        "###定义单步参数更新"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPZXQb9OwL-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's play the min-max game\n",
        "if not os.path.exists(\"./gif/\"):\n",
        "   os.makedirs(\"./gif/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJtYeH0_hzzt",
        "colab_type": "text"
      },
      "source": [
        "单步迭代"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMrbTiLXBxpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_step(images):\n",
        "  \n",
        "   # generating noise from a normal distribution\n",
        "  masked_imgs,missing_parts,real_parts,(y1, y2, x1, x2) = mask_randomly(np.array (images))\n",
        "\n",
        "  masked_imgs = tf.convert_to_tensor(np.array(masked_imgs),name = 'masked_imgs',dtype=tf.float32)\n",
        "  real_parts= tf.convert_to_tensor(np.array(real_parts),name = 'real_parts',dtype=tf.float32)\n",
        "    \n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    \n",
        "    generated_images = generator(masked_imgs)\n",
        "\n",
        "    real_output = discriminator(real_parts)\n",
        "    generated_output = discriminator(generated_images)\n",
        "\n",
        "    gen_loss = generator_loss(real_parts,generated_images,generated_output)\n",
        "    disc_loss = discriminator_loss(real_output, generated_output)\n",
        "\n",
        "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)\n",
        "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)\n",
        "  \n",
        "  del gen_tape\n",
        "  del disc_tape\n",
        "  \n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))\n",
        "  return real_parts, generated_images, gen_loss, disc_loss,(y1, y2, x1, x2),masked_imgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OeuNAYwAbGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator = mytrain_data.make_one_shot_iterator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9NYDIOSyZUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fig, axs = plt.subplots(1, 2, figsize=(9, 3), sharey=True)\n",
        "# for step in range(100):\n",
        "  \n",
        "#   try:\n",
        "#     mytrain_next = iterator.get_next()\n",
        "#   except tf.errors.OutOfRangeError:\n",
        "#     print(\"End of dataset\")\n",
        "#     break\n",
        "  \n",
        "\n",
        "\n",
        "#   if step % 20 == 0:\n",
        "#       print(\"G loss: \", g_loss_value.numpy(), \" D loss: \", d_loss_value.numpy(), \" step: \", step)\n",
        "#       show_and_save_images(real_part_data,fake_part_data,y1, y2, x1, x2, masked_imgs, fig, axs, step)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AurCyc2Z2oKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_and_save_images(real_data,fake_data,y1, y2, x1, x2, masked_imgs, fig, axs, name):\n",
        "  \n",
        "  real_data = np.squeeze(np.array(real_data))\n",
        "  fake_data = np.squeeze(np.array(fake_data))\n",
        "  masked_imgs = np.squeeze(np.array(masked_imgs))\n",
        "  imgs = masked_imgs.copy()\n",
        "  imgs[int(y1):int(y2), int(x1):int(x2), :]=real_data\n",
        "\n",
        "  axs[0].imshow(imgs)\n",
        "  axs[0].axis('off')\n",
        "  filled_in = masked_imgs.copy()\n",
        "  filled_in[int(y1):int(y2), int(x1):int(x2), :]=fake_data\n",
        "  axs[1].imshow(filled_in)\n",
        "  axs[1].axis('off')\n",
        "  \n",
        "  axes = plt.gca()\n",
        "  display.display(pl.gcf())\n",
        "  display.clear_output(wait=True)\n",
        "  plt.savefig(\"./gif/{}.png\".format(name))\n",
        "  \n",
        "#   plt.gca().clear()\n",
        "#   time.sleep(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwbuybAFh5Da",
        "colab_type": "text"
      },
      "source": [
        "定义保存图片"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUcK5cnHh9IK",
        "colab_type": "text"
      },
      "source": [
        "###定义训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNaQfLaBMNN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(mytrain_data,epochs,test_data,name):  \n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    iterator = mytrain_data.make_one_shot_iterator()\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(9, 3), sharey=True)\n",
        "    for step in range(4000):\n",
        "\n",
        "      try:\n",
        "        mytrain_next = iterator.get_next()\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        print(\"End of dataset\")\n",
        "        break\n",
        "\n",
        "      real_part_data, fake_part_data,g_loss_value, d_loss_value,(y1, y2, x1, x2),masked_imgs= train_step(mytrain_next)\n",
        "      \n",
        "      if step % 200 == 0:\n",
        "          print(\"G loss: \", g_loss_value.numpy(), \" D loss: \", d_loss_value.numpy(), \" name: \", name)\n",
        "          show_and_save_images(real_part_data,fake_part_data,y1, y2, x1, x2, masked_imgs, fig, axs, name)\n",
        "          name=name+1\n",
        "          \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zm4SPUdh_nj",
        "colab_type": "text"
      },
      "source": [
        "训练并保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCOpJXKONsbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "discriminator.load_weights('./discriminator.h5')\n",
        "generator.load_weights('./generator.h5')\n",
        "name=0\n",
        "\n",
        "  \n",
        "train(mytrain_data,100,test_data,name)\n",
        "  \n",
        "generator.save_weights('./generator.h5', save_format='h5')\n",
        "discriminator.save_weights('./discriminator.h5', save_format='h5')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}